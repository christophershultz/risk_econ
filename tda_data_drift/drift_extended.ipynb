{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, pdb, sys, pickle\n",
    "from gudhi import RipsComplex, SimplexTree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Design\n",
    "\n",
    "**Simulate Data Under Heterogeneous Scenarios**\n",
    "\n",
    "1. Baseline: generate a baseline dataset $X$ of size $N = 1000$ with $d=3$ features where each feature follows a standard normal distribution. Generate Y as a linear combination of each X. Focus on various magnitudes.  \n",
    "2. Scenario 1: Covariate Shift: simulate covariate shift by altering the distribution of one or more features. E.g. change x1 from N(0,1) to N(0,2) after a certain time point T. \n",
    "3. Scenario 2: Concept Shift: simulate concept shift by changing the relationship. E.g. change the underlying function Y = \\sum(X) to Y = x1^2 + x2 + x3. \n",
    "4. Scenario 3: Mixed Drift: Combine both forms by changing x1 to N(0,2) and Y to the new function.\n",
    "5. Scenario 4: Linearly change the mean of X1 over time by gradually multiplying it by an increasing factor. \n",
    "\n",
    "**Consider PE and Traditional Methods**\n",
    "1. Traditional: KS Test, Chi-Squared, Population Stability Index\n",
    "2. TDA-PE\n",
    "\n",
    "**Demonstrate Robustness**\n",
    "1. Tweak hyperparameters (distance metrics, window size, etc.) \n",
    "\n",
    "**Compare time-to-identification by metric** \n",
    "1. Show that time to identification is better with TDA than it is with traditional metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "df = pd.DataFrame({'x1': np.random.normal(loc=0, scale=1, size=1000),\n",
    "                   'x2': np.random.normal(loc=0, scale=1, size=1000),\n",
    "                   'x3': np.random.normal(loc=0, scale=1, size=1000)})\n",
    "df['y'] = df['x1'] + df['x2'] + df['x3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject variance-drift scenarios\n",
    "chg_var = {} \n",
    "chgAmts = [0.0001, 0.001, 0.01, 0.1, 0.6, 1, 5, 10, 20, 30]\n",
    "for c in chgAmts: \n",
    "    tmp = df.copy(deep = True)\n",
    "    new = pd.DataFrame({'x1': np.random.normal(loc=0*c, scale=1*c, size=400),\n",
    "                    'x2': np.random.normal(loc=0, scale=1, size=400),\n",
    "                    'x3': np.random.normal(loc=0, scale=1, size=400)})\n",
    "    new['y'] = new['x1'] + new['x2'] + new['x3']\n",
    "    chg_var[c] = pd.concat([tmp, new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject covariance shift scenarios\n",
    "chg_cov_shift = {} \n",
    "chgs = ['x^2', 'x^3', 'x^4', 'x^0.5', 'x^0.25', '2x']\n",
    "for c in chgs: \n",
    "    tmp = df.copy(deep = True)\n",
    "    new = pd.DataFrame({'x1': np.random.normal(loc=0, scale=1, size=400),\n",
    "                   'x2': np.random.normal(loc=0, scale=1, size=400),\n",
    "                   'x3': np.random.normal(loc=0, scale=1, size=400)})\n",
    "    if c == 'x^2': new['y'] = new['x1']**2 + new['x2'] + new['x3']\n",
    "    elif c == 'x^3': new['y'] = new['x1']**3 + new['x2'] + new['x3']\n",
    "    elif c == 'x^4': new['y'] = new['x1']**4 + new['x2'] + new['x3']\n",
    "    elif c == 'x^0.5': new['y'] = new['x1']**0.5 + new['x2'] + new['x3']\n",
    "    elif c == 'x^0.25': new['y'] = new['x1']**0.25 + new['x2'] + new['x3']\n",
    "    else: new['y'] = 2*new['x1'] + new['x2'] + new['x3']\n",
    "    chg_cov_shift[c] = pd.concat([tmp, new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject Mixed Drift\n",
    "chg_mix_shift = {} \n",
    "chgs = [0.001, 0.1, 5, 30]\n",
    "for c in chgAmts: \n",
    "    tmp = df.copy(deep = True)\n",
    "    new = pd.DataFrame({'x1': np.random.normal(loc=0*c, scale=1*c, size=400),\n",
    "                    'x2': np.random.normal(loc=0, scale=1, size=400),\n",
    "                    'x3': np.random.normal(loc=0, scale=1, size=400)})\n",
    "    new['y'] = new['x1']**0.5 + new['x2'] + new['x3']\n",
    "    chg_mix_shift[c] = pd.concat([tmp, new])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 0.0001\n",
      "W: 30\n",
      "K: 0.001\n",
      "W: 30\n",
      "K: 0.01\n",
      "W: 30\n",
      "K: 0.1\n",
      "W: 30\n",
      "K: 0.6\n",
      "W: 30\n",
      "K: 1\n",
      "W: 30\n",
      "K: 5\n",
      "W: 30\n",
      "K: 10\n",
      "W: 30\n",
      "K: 20\n",
      "W: 30\n",
      "K: 30\n",
      "W: 30\n",
      "K: x^2\n",
      "W: 30\n",
      "K: x^3\n",
      "W: 30\n",
      "K: x^4\n",
      "W: 30\n",
      "K: x^0.5\n",
      "W: 30\n",
      "K: x^0.25\n",
      "W: 30\n",
      "K: 2x\n",
      "W: 30\n",
      "K: 0.0001\n",
      "W: 30\n",
      "K: 0.001\n",
      "W: 30\n",
      "K: 0.01\n",
      "W: 30\n",
      "K: 0.1\n",
      "W: 30\n",
      "K: 0.6\n",
      "W: 30\n",
      "K: 1\n",
      "W: 30\n",
      "K: 5\n",
      "W: 30\n",
      "K: 10\n",
      "W: 30\n",
      "K: 20\n",
      "W: 30\n",
      "K: 30\n",
      "W: 30\n"
     ]
    }
   ],
   "source": [
    "dicts = {'chg_var': chg_var, 'cov_shift': chg_cov_shift, 'mix_shift': chg_mix_shift}\n",
    "results = {} \n",
    "for d in dicts.keys(): \n",
    "    name = d\n",
    "    chg = dicts[d]\n",
    "    res = {} \n",
    "    for k in chg.keys(): \n",
    "        for w in [30]: \n",
    "            print(\"K: \" + str(k))\n",
    "            print(\"W: \" + str(w))\n",
    "            # Combine df and df_drifted\n",
    "            df_combined = chg[k]\n",
    "\n",
    "            # Generate a time index\n",
    "            df_combined['time'] = np.arange(len(df_combined))\n",
    "\n",
    "            # Create a function to compute persistence diagrams\n",
    "            def compute_persistence_diagrams(data):\n",
    "                rips_complex = RipsComplex(points=data)\n",
    "                simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "                simplex_tree.persistence()\n",
    "                persistence_intervals = simplex_tree.persistence_intervals_in_dimension(1)\n",
    "                return persistence_intervals\n",
    "\n",
    "            # Create a function to compute persistence entropy\n",
    "            def compute_persistence_entropy(diagrams):\n",
    "                entropies = []\n",
    "                for diagram in diagrams:\n",
    "                    lifetimes = diagram[:, 1] - diagram[:, 0]\n",
    "                    lifetimes = lifetimes[lifetimes > 0]\n",
    "                    if len(lifetimes) > 0:\n",
    "                        p = lifetimes / lifetimes.sum()\n",
    "                        entropy = -np.sum(p * np.log(p))\n",
    "                    else:\n",
    "                        entropy = 0\n",
    "                    entropies.append(entropy)\n",
    "                return entropies\n",
    "\n",
    "            # Compute persistence entropy over time\n",
    "            entropy_values = []\n",
    "\n",
    "            for start in range(len(df_combined) - w + 1):\n",
    "                window_data = df_combined[['x1', 'x2', 'x3', 'y']].iloc[start:start + w].values\n",
    "                persistence_diagrams = [compute_persistence_diagrams(window_data)]\n",
    "                entropy_value = compute_persistence_entropy(persistence_diagrams)[0]\n",
    "                entropy_values.append(entropy_value)\n",
    "            df_combined['pe'] = [0]*(w-1) + entropy_values\n",
    "            dfc = df_combined.copy(deep = True)\n",
    "            res[str(k) + '-' + str(w)] = dfc\n",
    "    results[name] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_2.pkl', 'wb') as f: \n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 0.0001\n",
      "W: 30\n",
      "K: 0.001\n",
      "W: 30\n",
      "K: 0.01\n",
      "W: 30\n",
      "K: 0.1\n",
      "W: 30\n",
      "K: 0.6\n",
      "W: 30\n",
      "K: 1\n",
      "W: 30\n",
      "K: 5\n",
      "W: 30\n",
      "K: 10\n",
      "W: 30\n",
      "K: 20\n",
      "W: 30\n",
      "K: 30\n",
      "W: 30\n",
      "K: x^2\n",
      "W: 30\n",
      "K: x^3\n",
      "W: 30\n",
      "K: x^4\n",
      "W: 30\n",
      "K: x^0.5\n",
      "W: 30\n",
      "K: x^0.25\n",
      "W: 30\n",
      "K: 2x\n",
      "W: 30\n",
      "K: 0.0001\n",
      "W: 30\n",
      "K: 0.001\n",
      "W: 30\n",
      "K: 0.01\n",
      "W: 30\n",
      "K: 0.1\n",
      "W: 30\n",
      "K: 0.6\n",
      "W: 30\n",
      "K: 1\n",
      "W: 30\n",
      "K: 5\n",
      "W: 30\n",
      "K: 10\n",
      "W: 30\n",
      "K: 20\n",
      "W: 30\n",
      "K: 30\n",
      "W: 30\n"
     ]
    }
   ],
   "source": [
    "from gtda.diagrams import PairwiseDistance\n",
    "# WASSERSTEIN\n",
    "\n",
    "dicts = {'chg_var': chg_var, 'cov_shift': chg_cov_shift, 'mix_shift': chg_mix_shift}\n",
    "results = {} \n",
    "for d in dicts.keys(): \n",
    "    name = d\n",
    "    chg = dicts[d]\n",
    "    res = {} \n",
    "    for k in chg.keys(): \n",
    "        for w in [30]: \n",
    "            print(\"K: \" + str(k))\n",
    "            print(\"W: \" + str(w))\n",
    "            # Combine df and df_drifted\n",
    "            df_combined = chg[k]\n",
    "\n",
    "            # Generate a time index\n",
    "            df_combined['time'] = np.arange(len(df_combined))\n",
    "\n",
    "            # Create a function to compute persistence diagrams\n",
    "            def compute_persistence_diagrams(data):\n",
    "                rips_complex = RipsComplex(points=data)\n",
    "                simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "                simplex_tree.persistence()\n",
    "                persistence_intervals = simplex_tree.persistence_intervals_in_dimension(1)\n",
    "                return persistence_intervals\n",
    "\n",
    "        def compute_wasserstein_distances(diagrams):\n",
    "            wasserstein_distances = []\n",
    "            if len(diagrams) < 2: return wasserstein_distances  # Return an empty list if there aren't enough diagrams to compare\n",
    "            \n",
    "            pairwise_distance = PairwiseDistance(metric='wasserstein')\n",
    "            for i in range(1, len(diagrams)):\n",
    "                diagram_1 = diagrams[i - 1]\n",
    "                diagram_2 = diagrams[i]\n",
    "        \n",
    "                # Compute the Wasserstein distance between diagram_1 and diagram_2\n",
    "                distance = pairwise_distance.fit_transform([diagram_1, diagram_2])[0, 1]\n",
    "                wasserstein_distances.append(distance)\n",
    "            return wasserstein_distances\n",
    "\n",
    "        # Compute WD over time\n",
    "        wasserstein_values = []\n",
    "\n",
    "        for start in range(len(df_combined) - w + 1):\n",
    "            window_data = df_combined[['x1', 'x2', 'x3', 'y']].iloc[start:start + w].values\n",
    "            persistence_diagrams = [compute_persistence_diagrams(window_data)]\n",
    "            wds = compute_wasserstein_distances(persistence_diagrams)\n",
    "            wasserstein_values.append(wds)\n",
    "        df_combined['wd'] = [0]*(w-1) + wasserstein_values\n",
    "        dfc = df_combined.copy(deep = True)\n",
    "        res[str(k) + '-' + str(w)] = dfc\n",
    "    results[name] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_wd.pkl', 'wb') as f: \n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
